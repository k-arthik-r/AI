{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k-arthik-r/AI/blob/main/Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8aU8xfhPfLI",
        "outputId": "0e7b8bec-2374-4f7c-b76a-e9c5b76812c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs1ORWvUdUTs",
        "outputId": "dc30e24c-ea73-44ac-8012-ad2f9dbfa05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/translation.zip\n",
            "   creating: /content/en-indic/\n",
            "   creating: /content/en-indic/final_bin/\n",
            "  inflating: /content/en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: /content/en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: /content/en-indic/final_bin/preprocess.log  \n",
            "  inflating: /content/en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "   creating: /content/en-indic/model/\n",
            "  inflating: /content/en-indic/model/checkpoint_best.pt  \n",
            "   creating: /content/en-indic/vocab/\n",
            "  inflating: /content/en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: /content/en-indic/vocab/bpe_codes.32k.TGT  \n",
            "  inflating: /content/en-indic/vocab/vocab.SRC  \n",
            "  inflating: /content/en-indic/vocab/vocab.TGT  \n",
            "   creating: /content/inference/\n",
            "  inflating: /content/inference/custom_interactive.py  \n",
            "  inflating: /content/inference/engine.py  \n",
            " extracting: /content/inference/__init__.py  \n",
            "   creating: /content/inference/__pycache__/\n",
            "  inflating: /content/inference/__pycache__/custom_interactive.cpython-39.pyc  \n",
            "  inflating: /content/inference/__pycache__/engine.cpython-39.pyc  \n",
            "  inflating: /content/inference/__pycache__/__init__.cpython-39.pyc  \n",
            "   creating: /content/model_configs/\n",
            "  inflating: /content/model_configs/custom_transformer.py  \n",
            " extracting: /content/model_configs/__init__.py  \n",
            "   creating: /content/model_configs/__pycache__/\n",
            "  inflating: /content/model_configs/__pycache__/custom_transformer.cpython-39.pyc  \n",
            "  inflating: /content/model_configs/__pycache__/__init__.cpython-39.pyc  \n",
            "   creating: /content/scripts/\n",
            "  inflating: /content/scripts/add_joint_tags_translate.py  \n",
            "  inflating: /content/scripts/add_tags_translate.py  \n",
            "  inflating: /content/scripts/clean_vocab.py  \n",
            "  inflating: /content/scripts/concat_joint_data.py  \n",
            "  inflating: /content/scripts/extract_non_english_pairs.py  \n",
            "  inflating: /content/scripts/postprocess_score.py  \n",
            "  inflating: /content/scripts/postprocess_translate.py  \n",
            "  inflating: /content/scripts/preprocess_translate.py  \n",
            "  inflating: /content/scripts/remove_large_sentences.py  \n",
            "  inflating: /content/scripts/remove_train_devtest_overlaps.py  \n",
            " extracting: /content/scripts/__init__.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/translation.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFuxHX7OeC6Y",
        "outputId": "0f4d29fe-7380-46f4-c4f8-9496dd2dba1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Collecting indicnlp\n",
            "  Downloading indicnlp-0.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.8)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Collecting mock (from subword-nmt)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.5.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.11.17)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (23.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291820 sha256=11f840ccfe3dbaf8cd858ece971cb86eb0c01e8d60b793af42a11f8d69bd5911\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=56e192d01b71357273c0e296c7f34252af33eed3272543cf88b2f1e78635a432\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: morfessor, indicnlp, bitarray, antlr4-python3-runtime, sacremoses, portalocker, omegaconf, mock, colorama, subword-nmt, sacrebleu, hydra-core, sphinxcontrib-jquery, sphinx-argparse, flask-ngrok, sphinx-rtd-theme, fairseq, indic-nlp-library\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 flask-ngrok-0.0.25 hydra-core-1.0.7 indic-nlp-library-0.92 indicnlp-0.0.1 mock-5.1.0 morfessor-2.0.6 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0 sacremoses-0.1.1 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 subword-nmt-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses fairseq flask-ngrok indicnlp subword-nmt indic-nlp-library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ggLaUqrfgIX",
        "outputId": "dc8e1890-c02e-42a9-f845-f3f7d9498afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from mosestokenizer)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49170 sha256=7f866031b7741636402facd450fd098adb5f61b310999f56be0216e876ce73e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=f0e501308576fe0a538b77fbe142151fa555171c8bbfda881c5d908e283ee35d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3338 sha256=2805ea6daae2b8805ecbc27989e1299fe1717b99d2d29237d444cc5824e2e8eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6147 sha256=79c44bf767d112553bfa74fd4c040a07496959c3eabcc9757b0b038f0729dbf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mosestokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uAElZbLghev",
        "outputId": "e07badac-fe78-42ae-9c8b-c9fcd77bfa52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m627.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2023.11.17)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.1.0+cu121\n",
            "    Uninstalling torchaudio-2.1.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU8X-hhdhLQW",
        "outputId": "ea843032-15a8-4ef1-96e5-f8803b0c5ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairseq==0.12.2 in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.8)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.1)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.9.2)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install fairseq==0.12.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNy1rH29nmMB",
        "outputId": "67d3e728-87f3-4cfe-dd0a-a73c8fb291a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-21 04:55:47--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  5.83MB/s    in 2.3s    \n",
            "\n",
            "2024-01-21 04:55:51 (5.83 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNxLWNdxGz4U",
        "outputId": "9218e4cd-0c35-4a0c-bc9f-7921caedde9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!./ngrok authtoken '2MaVZRc6ZdR00NbLyQr8QNqs8Ys_7sKWefcnSs38rgCy8EoWf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP_FBKaaHKZy",
        "outputId": "d2cb736f-5094-4e8d-d2ac-26d0f3f3cabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.5-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI9Pykgfg5LX",
        "outputId": "1c944cff-1ccb-4944-cb33-b9e90188e883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, render_template, request, jsonify\n",
        "from inference.engine import Model\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "indic2en_Model = Model(expdir='en-indic')\n",
        "\n",
        "@app.route('/')\n",
        "def hello():\n",
        "    return \"Hello, Flask on Google Colab!\"\n",
        "\n",
        "@app.route('/translate', methods=['POST'])\n",
        "def translate():\n",
        "    data = request.get_json()\n",
        "    texts = data.get('texts', [])\n",
        "    target_lang = data.get('target_lang', 'en')\n",
        "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
        "    return jsonify({'translated_texts': translated_texts})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EU5x28z_Hta",
        "outputId": "554b1bc0-d0fd-414a-812a-d3b12d6ca6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://ae69-34-91-91-194.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 1104.29it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:08:11] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 50/50 [00:00<00:00, 2164.11it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:08:23] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 8/8 [00:00<00:00, 1480.19it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:08:28] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 8/8 [00:00<00:00, 828.89it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:09:11] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 6775.94it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:09:18] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 4941.45it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:09:28] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 8160.12it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:09:35] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 50/50 [00:00<00:00, 2166.73it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:09:46] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 7309.70it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:41:32] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 3444.16it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:41:35] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 5146.39it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:41:38] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 5227.20it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:41:41] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 50/50 [00:00<00:00, 1214.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Sentence __src__en__ __tgt__kn__ ಹ@@ ೇ ! .... ್@@ ತ@@ ದ@@ ೆ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ವ@@ ೈ@@ ದ@@ .... CL@@ IC@@ K H@@ ERE truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ವ@@ ೈ@@ ದ@@ .... ಕ@@ ಿ@@ ಸ@@ ಿ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ರ@@ ೋ@@ ಗ .... CL@@ IC@@ K H@@ ERE truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ರ@@ ೋ@@ ಗ .... ್@@ ತ@@ ದ@@ ೆ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ನ@@ ೇ@@ ಮ@@ .... CL@@ IC@@ K H@@ ERE truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ಅ@@ ಪ@@ ಾ@@ .... ್@@ ತ@@ ದ@@ ೆ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ &@@ quot@@ ; .... ೆ . &@@ quot@@ ; truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ನ@@ ೀ@@ ವ@@ .... ತ@@ ಿ@@ ಪ@@ ರ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ಕ@@ ು@@ ಟ@@ .... ಚ@@ ಿ@@ ಕ@@ ೆ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ಮ@@ ಾ@@ ನ@@ .... ಹ@@ ು@@ ದ@@ ು . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ದ@@ ೈ@@ ಹ@@ .... ಗ@@ ಿ@@ ದ@@ ೆ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ನ@@ ೀ@@ ವ@@ .... ಸ@@ ಹ@@ ಾ@@ ಯ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ನ@@ ಿ@@ ಮ@@ .... ರ@@ ಿ@@ ಯ@@ ೆ . truncated to 200 tokens as it exceeds maximum length limit\n",
            "WARNING: Sentence __src__en__ __tgt__kn__ ಮ@@ ಿ@@ ದ@@ .... ೆ@@ ಗ@@ ಳ@@ ು . truncated to 200 tokens as it exceeds maximum length limit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 03:42:02] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 5/5 [00:00<00:00, 3286.56it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:22:14] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 8/8 [00:00<00:00, 1939.79it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:22:19] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 2/2 [00:00<00:00, 572.29it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 6326.01it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:22:23] \"POST /translate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:22:23] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 2/2 [00:00<00:00, 1216.09it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:22:33] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 2/2 [00:00<00:00, 717.04it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 6250.14it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:24:00] \"POST /translate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:24:00] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 81/81 [00:00<00:00, 8413.95it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:25:53] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 81/81 [00:00<00:00, 6016.91it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:25:56] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 81/81 [00:00<00:00, 4654.02it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:24] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 81/81 [00:00<00:00, 3460.29it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 3807.15it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 1453.39it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 1209.37it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 1254.37it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 1220.97it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 2079.12it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 4392.00it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 1002.26it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 1277.17it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 1759.71it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 4305.78it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 2457.58it/s]\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 348, in _generate\n",
            "    encoder_outs = self.model.reorder_encoder_out(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 890, in reorder_encoder_out\n",
            "    model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 363, in reorder_encoder_out\n",
            "    encoder_out[\"encoder_embedding\"][0].index_select(0, new_order)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.32 GiB already allocated; 9.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 354, in _generate\n",
            "    lprobs, avg_attn_scores = self.model.forward_decoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 819, in forward_decoder\n",
            "    decoder_out = model.decoder.forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
            "    x, extra = self.extract_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
            "    return self.extract_features_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\n",
            "    x, layer_attn, _ = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 641, in forward\n",
            "    x, attn = self.encoder_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/multihead_attention.py\", line 594, in forward\n",
            "    v = self.v_proj(key)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.29 GiB already allocated; 11.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 279, in _generate\n",
            "    encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 890, in reorder_encoder_out\n",
            "    model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 352, in reorder_encoder_out\n",
            "    new_encoder_out = [encoder_out[\"encoder_out\"][0].index_select(1, new_order)]\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.35 GiB already allocated; 11.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 354, in _generate\n",
            "    lprobs, avg_attn_scores = self.model.forward_decoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 819, in forward_decoder\n",
            "    decoder_out = model.decoder.forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
            "    x, extra = self.extract_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
            "    return self.extract_features_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\n",
            "    x, layer_attn, _ = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 641, in forward\n",
            "    x, attn = self.encoder_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/multihead_attention.py\", line 593, in forward\n",
            "    k = self.k_proj(key)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.31 GiB already allocated; 9.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 354, in _generate\n",
            "    lprobs, avg_attn_scores = self.model.forward_decoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 819, in forward_decoder\n",
            "    decoder_out = model.decoder.forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
            "    x, extra = self.extract_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
            "    return self.extract_features_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\n",
            "    x, layer_attn, _ = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 641, in forward\n",
            "    x, attn = self.encoder_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/multihead_attention.py\", line 593, in forward\n",
            "    k = self.k_proj(key)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.31 GiB already allocated; 9.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 279, in _generate\n",
            "    encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 890, in reorder_encoder_out\n",
            "    model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 352, in reorder_encoder_out\n",
            "    new_encoder_out = [encoder_out[\"encoder_out\"][0].index_select(1, new_order)]\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.29 GiB already allocated; 9.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 354, in _generate\n",
            "    lprobs, avg_attn_scores = self.model.forward_decoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 819, in forward_decoder\n",
            "    decoder_out = model.decoder.forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
            "    x, extra = self.extract_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
            "    return self.extract_features_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\n",
            "    x, layer_attn, _ = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 641, in forward\n",
            "    x, attn = self.encoder_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/multihead_attention.py\", line 593, in forward\n",
            "    k = self.k_proj(key)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.32 GiB already allocated; 9.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 354, in _generate\n",
            "    lprobs, avg_attn_scores = self.model.forward_decoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 819, in forward_decoder\n",
            "    decoder_out = model.decoder.forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
            "    x, extra = self.extract_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
            "    return self.extract_features_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\n",
            "    x, layer_attn, _ = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 641, in forward\n",
            "    x, attn = self.encoder_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/multihead_attention.py\", line 593, in forward\n",
            "    k = self.k_proj(key)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.32 GiB already allocated; 9.06 MiB free; 13.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 419, in _generate\n",
            "    cand_scores, cand_indices, cand_beams = self.search.step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/search.py\", line 126, in step\n",
            "    lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 14.75 GiB total capacity; 13.41 GiB already allocated; 21.06 MiB free; 13.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 419, in _generate\n",
            "    cand_scores, cand_indices, cand_beams = self.search.step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/search.py\", line 126, in step\n",
            "    lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 14.75 GiB total capacity; 13.41 GiB already allocated; 21.06 MiB free; 13.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "ERROR:__main__:Exception on /translate [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-10-fd5f03870d15>\", line 20, in translate\n",
            "    translated_texts = indic2en_Model.batch_translate(texts, 'en', target_lang)\n",
            "  File \"/content/inference/engine.py\", line 106, in batch_translate\n",
            "    translations = self.translator.translate(tagged_sents)\n",
            "  File \"/content/inference/custom_interactive.py\", line 254, in translate\n",
            "    translations = self.task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 348, in _generate\n",
            "    encoder_outs = self.model.reorder_encoder_out(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 890, in reorder_encoder_out\n",
            "    model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 352, in reorder_encoder_out\n",
            "    new_encoder_out = [encoder_out[\"encoder_out\"][0].index_select(1, new_order)]\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 13.31 GiB already allocated; 81.06 MiB free; 13.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:30] \"\u001b[35m\u001b[1mPOST /translate HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:32] \"POST /translate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:27:32] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 81/81 [00:00<00:00, 4093.29it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:33:24] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 81/81 [00:00<00:00, 5658.63it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:33:33] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 81/81 [00:00<00:00, 8238.48it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:38:02] \"POST /translate HTTP/1.1\" 200 -\n",
            "100%|██████████| 2/2 [00:00<00:00, 502.97it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Jan/2024 04:38:07] \"POST /translate HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m4qO-d4PFUz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}